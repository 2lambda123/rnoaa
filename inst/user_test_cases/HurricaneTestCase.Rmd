---
title: 'Test Case: Hurricane Data'
author: Brooke Anderson (BA), Rachel Severson (RS), Casey Patrizio (CP), Mike Lyons (ML), Enqun Wang (EW)
output: html_document
---

```{r setup, include=FALSE, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(lubridate)
```

# Main task

Here are some recent tropical storms that were very severe with severe impacts to humans: 

Storm | Landfall  time | Landfall location | Some places affected
----- | ------------- | -----------------  | -------
Hurricane Andrew | 0905 UTC 24 August 1992 | near Homestead AFB FL | Miami, Key Largo, Nassau (Bahamas)
Labor Day Storm | late on September 2, 1935 | Florida Keys | Florida Keys, St. Petersburg, Florida
Cyclone Tracy | very early December 25, 1974 | Darwin, Australia | Darwin
Tropical Storm Bilis | 12:50 p.m. local time July 14, 2006 | near Fuzhou, China | Guangdong, Hunan, Fujian

You will be trying to collect relevant data for one of these storms. 

# Relevant NOAA weather products

In later sections, you will have specific goals to try to achieve, in terms of what data to get. However, as you work on this project, please keep a running list here of any of the NOAA data products that you think might have data that could be used to explore the storm you're working on. Include the name of the dataset, a short description of what you might be able to get from it, and a link to any websites that give more information.

- BA: The [IBTracS](http://www.ncdc.noaa.gov/ibtracs/index.php) dataset gives data on tropical storm tracks. It can be pulled through the `rnoaa` function `storm_data`. 
- BA: The [GHCN-Daily dataset](https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-ghcn) has daily measurements of precipitation and, for some monitors, temperature and wind speed, internationally. It sounds like coverage is best in the US, Australia, and Canada, but there are monitors worldwide. Several `rnoaa` functions work with this data: `meteo_nearby_stations` (find all the monitors within a certain radius of a lat-lon, or find the [x] closest monitors to a point), `meteo_pull_monitors`, and `ghcnd_search`. The function `weather_fips` in `countyweather` (a package Rachel's developing on GitHub) can pull and aggregate data from this source from all monitors in a county based on the county's FIPS code. 

BA: Note: For many of the `rnoaa` functions, you will need an API key from NOAA. Here's how you should save that on your computer so that you can get this document to compile without including your API key (these instructions are adapted from [here](https://cran.r-project.org/web/packages/httr/vignettes/api-packages.html):

1. Ask NOAA for an API key: http://www.ncdc.noaa.gov/cdo-web/token
2. Open a new text file. Put the following in it (note: this might not show up here, but there should be a second blank line):

```
noaakey == [your NOAA API key]

```

3. Save this text file as `.Renviron` in your home directory. Your computer might be upset that the file name starts with a dot. That's okay-- it should; ignore the warning and save like this. 
4. Restart R.
5. Now you can pull your key using `Sys.getenv("noaakey")`

Now you can gear up to use functions that require the API key: 

```{r}
options("noaakey" = Sys.getenv("noaakey"))
```

# Relevant other data sources

As you work on this project, also keep a running list here of any data from sources other than NOAA that you think might have data that could be used to explore the storm you're working on. Include the name of the dataset, a short description of what you might be able to get from it, and a link to any websites that give more information and / or the data itself.

- BA: The USGS data on streamflow could be relevant for assessing flooding. This is available through the `waterData` package in R. 

# Specific tasks

As you work on these, include the code that you try. Include things that worked and things that you thought would work but that did not. Also write down what parts were easy to figure out how to do, and which you had to search around a lot to figure out what to do. 

## Winds at landfall

Get any data you can that gives measurements of winds when the storm made landfall (use the landfall listed for the storm in the table-- often, there are more than one for a storm for different locations, but just try for the listed one). 

Try to get:

- A measure of how strong winds were over land around where the storm made landfall
- A measure of how strong winds were over water around where the storm made landfall
- Wind directions at different locations (on land or over water) near landfall 
- An estimate of how many of the stations near the landfall that were operating the week before the storm that were still operational and recording data at landfall

#### A measure of how strong winds were over land around where the storm made landfall

BA: It may work, for daily wind speeds, to use the Global Historical Climatology Network Daily data. If you have monitor ids, you can use the `meteo_pull_monitors` function from `rnoaa` to pull data from this source. If you have the FIPS code for a county, you can use the function `fips_stations` in `countyweather` (a package Rachel is developing) to get the station IDs for all relevant stations in a county. 

CP: I had issues running `install_github("ropenscilabs/rnoaa")` in order to use the function `meteo_pull_monitors`, so instead try `load_all()`. Note that this will only work if you have devtools installed and you have forked the rnoaa respository from Github.  
For example, the FIPS for Miami-Dade is 12086. Based on the [README file](http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt) for this data, some of the windspeed variables have the abbreviations: "AWND", "WSFG", "WSF1". You can therefore run:

```{r}
library(devtools)
install_github("ropenscilabs/rnoaa", dependencies=TRUE) # if you need to install the package
install_github("leighseverson/countyweather") # if you need to install the package
library(rnoaa)
library(countyweather)
miami_stations <- fips_stations("12086", date_min = "1992-08-01",
                                date_max = "1992-09-01")
miami_stations
```

This does not pull any weather stations. However, I know that the station "USW00012839" should exist for this time-- it's the Miami Intl Airport station. I wonder why this isn't picked up by `fips_stations`?

Once you know the station number, you can run: 

```{r fig.width = 5, fig.height = 3}
miami_wind <- meteo_pull_monitors("USW00012839", date_min = "1992-08-01",
                                     date_max = "1992-09-01", 
                                     var = c("AWND", "WSFG", "WSF1")) %>%
  mutate(wsfg = wsfg / 10) # Convert to m / s-- see the README for this data from link
head(miami_wind)
ggplot(miami_wind, aes(x = date, y = wsfg)) + 
  geom_line() + ggtitle("Wind gust speeds near Miami, FL, \nduring Hurricane Andrew") + 
  ylab("Wind gust (m / s)") 
```


It seems like we should be able to get wind data from some of the other NOAA data sources. In particular, it seems like we should be able to get hourly, or another fine temporal resolution, of wind data. Check the "Data sources" section of the file at the bottom of the page [here](https://github.com/ropenscilabs/rnoaa).

RS: The `isd()` function from the rnoaa package looks like it's getting hourly data. Here's an example with the station at Miami International Airport.


CP: A current issue is that the `isd()` function takes USAF and WBAN station codes as input, whereas we would like to use FIPS codes (county codes) as input. My initial thought for a way around this is to use the `isd_stations_search` from the rnoaa package, which takes lat/lon and radius (km) as input. I can use census data from [here](http://www2.census.gov/geo/docs/reference/cenpop2010/county/CenPop2010_Mean_CO.txt) to find population weighted lat/lon associated with a given FIPS code, and then use the lat/lon as input to `isd_stations_search` to get the stations associated with that FIPS code. Note that we would like to produce output similar to `weather_fips`, a function that takes a county code as input and then gives weather data in the surrounding area. `weather_fips`, except for hourly data. The `weather_fips` code can be forked from [here](https://github.com/leighseverson/countyweather).



```{r}
library(stringr)

census_data <- read.csv('http://www2.census.gov/geo/docs/reference/cenpop2010/county/CenPop2010_Mean_CO.txt')
state <- census_data$STATEFP
county <- census_data$COUNTYFP

state[str_length(state) == 1] <- paste0(0, state[str_length(state) == 1])
county[str_length(county) == 1] <- paste0(00, county[str_length(county) == 1])
county[str_length(county) == 2] <- paste0(0, county[str_length(county) == 2])

FIPS <- paste0(state,county)
census_data$FIPS <- FIPS

lat <- census_data$LATITUDE
lon <- census_data$LONGITUDE

#the idea is to go from FIPS -> population weighted lat/lon -> station ID (USAF and WBAN) -> hourly weather data

#this is the FIPS for Miami-Dade 
FIPS_test = "12086"
row_num = which(grepl(FIPS_test, census_data$FIPS))

lat_FIPS = lat[row_num]
lon_FIPS = lon[row_num]

#Get the stations within a 50 km radius corresponding to the 
#population weighted lat/lon of FIPS_test.
#It is also possible to specify a bounding box, instead of radius here.
stations <- isd_stations_search(lat=lat_FIPS, lon=lon_FIPS, radius = 50)

#stations contains usaf and wban codes in the county given by FIPS_test.
USAF_codes = stations$usaf
WBAN_codes = stations$wban

#We can apply the isd() function to each of these stations to get the average hourly 'weather' data in the area. 

#here is just an example of what the data looks like for a single station (Miami International Airport)
res <- isd(usaf = USAF_codes[1], wban = WBAN_codes[1], year = 1992)$data
res$date_time <- ymd_hm(sprintf("%s %s", as.character(res$date), res$time))

res <- res %>% filter(temperature < 900) %>% select(usaf_station, wban_station, 
                                                    date_time, latitude, longitude, 
                                                    wind_direction, wind_speed, 
                                                    temperature)

ggplot(res, aes(date_time, temperature)) +
  geom_line() +
  facet_wrap(~usaf_station, scales = "free_x")
```
 
RS: We should be able to filter the res dataframe to the particular month we're interested in to get hourly wind data. We found the USAF and WBAN codes for Miami by searching [this](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-history.txt) text file of stations - there should be a better way to find USAF and WBAN codes for a particular location or station. 

CP: I found a function to go from location (lat/lon) to USAF and WBAN codes `isd_stations_search`, and use census data to go from FIPS to lat/lon. See above code. 

More info about this data can be found by going to NOAA's [land-based station data](https://www.ncdc.noaa.gov/data-access/land-based-station-data) site, then following the link for Integrated Surface Hourly Data base (3505) at the bottom of the page. For example, the readme.txt and ish-format-document.pdf files could be helpful. 

#### A measure of how strong winds were over water around where the storm made landfall

BA: For this, it seems like [buoy data](http://www.ndbc.noaa.gov) might work. The `buoy` function in `rnoaa` might work. I'm not sure how you could get station IDs through `rnoaa` for the buoy stations. I used the website to find a station close to Miami ("fwyf1"). Once you have a buoy id, you can do this kind of call: 

```{r fig.width = 6, fig.height = 2.5}
ex <- buoy(dataset = 'cwind', buoyid = "fwyf1", year = 1992)
head(ex$data)
fl_buoy <- ex$data %>% mutate(time = ymd_hms(time)) %>%
  filter(time > ymd_hms("1992-08-23 00:00:00") & 
           time < ymd_hms("1992-08-26 00:00:00"))
ggplot(fl_buoy, aes(x = time, y = wind_spd)) + geom_line()
```

It looks like this buoy was actually lost during the hurricane. It stops recording during the worst of the storm, and it doesn't look like it got back online in 1992 after that. 

It looks like you need to look through the historical station maps to find a station, rather than the current ones. If there's somewhere we could get a list of all buoy ids by latitude and longitude, we could identify the ones closest to a location.

#### Wind directions at different locations (on land or over water) near landfall 

#### An estimate of how many of the stations near the landfall that were operating the week before the storm that were still operational and recording data at landfall

## Precipitation at affected cities

For each of the affected cities, estimate the precipitation during the storm and on neighboring days. Include: 

- Daily and hourly estimates of rainfall
- How many stations you used to get each of those values
- A map of stations you used to get those values, with some measure on the map of the maximum daily or hourly rainfall measured at that station

#### Daily and hourly estimates of rainfall

RS: The countyweather package has some functions for pulling and aggreagating weather data by FIPS code. For the US storms, this might be helpful for this task. If you have devtools installed, you can install countyweather using the following code: 
```{r eval=FALSE}
# library(devtools)
# install_github("ropenscilabs/rnoaa") # if you need to install the package
# install_github("leighseverson/countyweather") # if you need to install the package
library(rnoaa)
library(countyweather)
miami_stations <- fips_stations(fips = "12086")
miami_rain <- meteo_pull_monitors(miami_stations, var = "PRCP")
range(miami_rain$date)
```

BG: For some reason, NCDC seems to only be identifying Miami weather monitors that operated from 2007 on. 

You can instead use the `meteo_nearby_stations` function to find all precipitation monitors in a certain radius (parts of this take a long time to run, so I won't evaluate):

```{r eval = FALSE}
station_data <- ghcnd_stations()[[1]] # Takes a while to run
miami <- data.frame(id = "miami", latitude = 25.7617, longitude = -80.1918)
miami_monitors <- meteo_nearby_stations(lat_lon_df = miami,
                                        station_data = station_data,
                                        radius = 50,
                                        var = c("PRCP"), 
                                        year_min = 1992, year_max = 1992)
miami_monitors <- miami_monitors[[1]]$id
```

```{r echo = FALSE}
miami_monitors <- c("USW00092811", "USW00012859", "USC00083909", "USW00012839",
                    "USC00087020", "USW00012849", "USC00083163", "USC00083168",
                    "USC00084095")
```

```{r fig.width = 8, fig.height = 3}
miami_weather <- meteo_pull_monitors(miami_monitors, date_min = "1992-08-01",
                                     date_max = "1992-09-01", 
                                     var = c("PRCP")) 
ggplot(miami_weather, aes(x = date, y = prcp, color = id)) + 
  geom_line() + ggtitle("Precipitation near Miami, FL, \nduring Hurricane Andrew") + 
  ylab("Precipitation (mm)")
```

#### How many stations you used to get each of those values

#### A map of stations you used to get those values, with some measure on the map of the maximum daily or hourly rainfall measured at that station

## Flooding related to the hurricane

Was there severe flooding related to the storm? For locations along the storm path, try to figure out: 

- What areas were along the storm path?
- Can you identify streams and rivers that were affected by the storm?
- What cities had problems with flooding during the storm? Get some measure to identify if and to what degree the location flooded.
- How closely were rainfall and flooding severity linked in locations?

#### What areas were along the storm path?

BG: For this, you'll need to have the storm track for each storm. I guess then match with distance to lat-lon coordinates for city or county centers to identify locations near the storm track?

The `rnoaa` package has some functions to tap into the [IBTracS](http://www.ncdc.noaa.gov/ibtracs/index.php) dataset, which has tropical storm tracks. It looks like you might be able to use the `storm_data` function to pull in data on a storm. You can pull for just one storm if you have the storm serial number, but it's not entirely clear to me how you would figure out what this number is for a storm you want to look for. 

You can pull by year or basin, but it looks like you can't pull by both at the same time. I'll put just for 1992:

```{r}
# install_github("ropenscilabs/rnoaa") # if you need to install the package
library(rnoaa)
hurrs_1992 <- storm_data(year = 1992)
```

It looks like that pulls a list object, with the element `data` that has the actual data: 

```{r}
names(hurrs_1992)
colnames(hurrs_1992$data)[1:10]
```

Maybe we could use the `name` column to look for Andrew?

```{r}
andrew <- hurrs_1992$data[which(hurrs_1992$data$name == "ANDREW"), ]
andrew[1:5, 1:10]
```

This looks about right... 

Note: after digging some more, it looks like you can use the `storm_meta` function with the argument `what = "storm_names"` from `storm_data` to get the serial number for a storm if you know its name: 

```{r}
storm_names <- storm_meta(what = "storm_names")
head(storm_names, 3)
grep("ANDREW", storm_names$name, value = TRUE)
storm_names[grep("ANDREW", storm_names$name), ]
andrew_id <- storm_names[grep("ANDREW", storm_names$name), 1][2]
andrew_2 <- storm_data(storm = andrew_id)
head(andrew_2$data[ , c("iso_time", "latitude", "longitude", "wind.wmo.")], 3)
```

Let's see what happens if we plot the locations of this:

```{r fig.width = 5, fig.height = 3}
andrew_loc <- select(andrew, iso_time, latitude, longitude, wind.wmo.)
head(andrew_loc, 3)
ggplot(andrew_loc, aes(x = longitude, y = latitude)) + 
  geom_path() + geom_point(aes(color = wind.wmo.))
```

Let me add to a map: 

```{r fig.width = 6, fig.height = 4, message = FALSE, warning = FALSE}
library(ggmap)
miami_map <- get_map(location = 'Miami', zoom = 3)
ggmap(miami_map) + 
  geom_path(data = andrew_loc, aes(x = longitude, y = latitude)) + 
  geom_point(data = andrew_loc, aes(x = longitude, y = latitude, 
                                    color = wind.wmo.))
```

And more locally: 

```{r fig.width = 6, fig.height = 4, message = FALSE, warning = FALSE}
miami_map <- get_map(location = 'Miami', zoom = 5)
ggmap(miami_map) + 
  geom_path(data = andrew_loc, aes(x = longitude, y = latitude)) + 
  geom_point(data = andrew_loc, aes(x = longitude, y = latitude, 
                                    color = wind.wmo.))
```

Next, we could measure distance from US county centers and pick out the counties that were within [x] kilometers of the 6-hour storm location measures.

Cyclone Tracy example: (note: need to pull from 1975, even though it was in 1974)

```{r, message=F}
Tracy_1975<-storm_data(year=1975)
names(Tracy_1975)
colnames(Tracy_1975$data)[1:10]
tracy <- Tracy_1975$data[which(Tracy_1975$data$name == "07S:TRACY"), ]
as.vector(unique(Tracy_1975$data$name))
tracy[1:5, 1:10]
storm_names <- storm_meta(what = "storm_names")
head(storm_names)
grep("TRACY", storm_names$name, value = TRUE)
storm_names[grep("TRACY", storm_names$name), ]
tracy_id <- storm_names[grep("TRACY", storm_names$name), 1][1]
tracy_2 <- storm_data(storm = tracy_id)
head(tracy_2$data[ , c("iso_time", "latitude", "longitude", "wind.wmo.")], 3)
tracy_loc <- select(tracy, iso_time, latitude, longitude, wind.wmo.)
tracy_loc$latitude[tracy_loc$latitude < -300]<-NA
tracy_loc$longitude[tracy_loc$longitude < -300]<-NA
tracy_loc$wind.wmo.[tracy_loc$wind.wmo. < -300]<-NA
head(tracy, 3)
ggplot(tracy_loc, aes(x = longitude, y = latitude)) +
  geom_path() + geom_point(aes(color = wind.wmo.))
library(ggmap)
darwin_map <- get_map(location = 'Darwin', zoom = 5)
ggmap(darwin_map) +
  geom_path(data = tracy_loc, aes(x = longitude, y = latitude)) +
  geom_point(data = tracy_loc, aes(x = longitude, y = latitude,
                                    color = wind.wmo.))
```

##### Storm Path Function

EW: I wrote a function to plot the storm path. You need to know the `name`, `year`, `location` of the storm you would like to analyze. The function provides two types of map: the whole map or the local map.

```{r message=F}
require(rnoaa)
require(ggplot2)

storm_path <- function(name = "BILIS", year = 2006, location = 'Fuzhou, China', 
                       local = TRUE){
  # name: name of the storm
  # year: year of the storm
  # location: location of the storm
  # local: if true, plot the local map; else, plot the whole plot
  
  hurrs_year <- storm_data(year = year)
  storm <- hurrs_year$data[which(hurrs_year$data$name == name), ] 
  if(sum(storm$latitude == -999)){
    storm <- storm[-c(which(storm$latitude == -999)),]
  }
  storm_loc <- select(storm, iso_time, latitude, longitude, wind.wmo.) 
  
  if(local){
    map <- get_map(location = location, zoom = 5) 
  }else{
    map <- get_map(location = location, zoom = 3, source='google')
  }
  ggmap(map) +
    geom_path(data = storm_loc, aes(x = longitude, y = latitude)) + 
    geom_point(data = storm_loc, aes(x = longitude, y = latitude, 
                                     color = wind.wmo.))
}
```

**Hurricane Andrew:**

```{r message=F}
storm_path(name = "ANDREW", year = 1992, location = "Miami", local = T)
```

**Labor Day Storm:**

```{r message=F}
storm_path(name = "XXXX935513", year = 1935, location = "Florida", local = F)
```

**Cyclone Tracy:** cannot find the data.

**Tropical Storm Bilis:**

```{r message=F}
storm_path(name = "BILIS", year = 2006, location = "Fuzhou", local = T)
```

#### Can you identify streams and rivers that were affected by the storm?

BG: No idea where to start here... Are there any databases where you can identify rivers or streams that are close to a certain location? Does streamgage meta-data have some information saying what river or stream it's on? If so, we might be able to pull meta-data on all streamgages through USGS, filter to just ones within the right date range, and then pull the latitude-longitude and the stream / river name, then calculate distances to the central path of the storm from storm tracks.

#### What cities had problems with flooding during the storm? Get some measure to identify if and to what degree the location flooded.

BG: The `waterData` package allows you to pull daily hydrologic time series from the USGS. You need to have the station IDs to do that first, though. 

The package `countyweather` (this is Rachel's package) has a function to pull all the station IDs for a US county, based on the county FIPS code. This function is still in development (so feel free to fork the package and make your own changes!), but for right now you can use it like this-- to get the station IDs for Miami-Dade, which has a FIPS code of 12086:

[Change to load from my branch of county weather for the moment because of the `streamstations` issue.]

```{r}
# library(devtools)
install_github("geanders/countyweather") # install the package if you need to
library(countyweather)
miami_ids <- countyweather:::streamstations("12086",
                                       date_min = "1992-08-20",
                                       date_max = "1992-08-30")
miami_ids
```

Then you can use these IDs with `importDVs` from the `waterData` package to import daily hydrological time series data. This function will, I think, only pull for one gauge at a time:

```{r fig.width = 4, fig.height = 2.5}
# install.packages(waterData) # if you need to install the package
library(waterData)
miami_1 <- importDVs(miami_ids[1], sdate = "1992-08-20", edate = "1992-08-30")
head(miami_1)
ggplot(miami_1, aes(x = dates, y = val)) + geom_line() + 
  geom_point() + ggtitle("Stream discharge for streamgage 02287395")
```

The default is for `importDVs` to pull in the mean daily value. You can use the `stat` option to change what you get in, although for this, you'd want to look up what all the USGS statistics codes are. You can also change the USGS parameter code using the `code` argument. The default is "00060", which is the discharge in cubic feet per second. The help file for `importDvs` has web links to find the right codes for these. 

So, I think next steps would be to use something from the `apply` family of functions or a loop to go through and get data from all of the Miami gauges, and then plot the discharge for them all. Are there any problems with pulling data from any of the gauges that we get back from the `streamflow` function?

#### How closely were rainfall and flooding severity linked in locations?

BG: One way to do this would be to try to get county-level aggregated daily estimates of rainfall and flooding intensity. You could do all sorts of things with this. For example, you could take the county maximum of both values for days near the storm and see how well correlated those are across different counties. You could also try to measure how many days are typical between the maximum rain fall and maximum stream flow.

## Plotting the storm

Plot the full track of the storm. Show its intensity as it progressed along the track, as well as the affected cities.

